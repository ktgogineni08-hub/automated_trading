# Alert Escalation Policy

**Trading System - Alert Management and Escalation**

Version: 1.0
Last Updated: November 2025
Owner: SRE Team

---

## Table of Contents

1. [Overview](#overview)
2. [Severity Levels](#severity-levels)
3. [Escalation Matrix](#escalation-matrix)
4. [Response Times (SLA)](#response-times-sla)
5. [Notification Channels](#notification-channels)
6. [On-Call Rotation](#on-call-rotation)
7. [Alert Categories](#alert-categories)
8. [Escalation Workflows](#escalation-workflows)
9. [Post-Incident Procedures](#post-incident-procedures)

---

## Overview

This document defines the escalation policy for alerts generated by the Trading System monitoring infrastructure. It ensures that all alerts are acknowledged, investigated, and resolved in a timely manner based on their severity and impact.

### Objectives

- Define clear escalation paths for different alert severities
- Ensure appropriate response times for critical incidents
- Minimize service disruption and trading losses
- Maintain comprehensive incident documentation

---

## Severity Levels

### P1 - Critical (ðŸ”´ RED)

**Impact:** Complete service outage or severe functional impairment affecting live trading

**Examples:**
- All trading system replicas down
- Database primary failure with no automatic failover
- Redis master down with no failover
- Authentication system failure
- Order execution pipeline broken

**Response Time:** 15 minutes
**Notification:** PagerDuty (immediate page), Slack, Email

---

### P2 - High (ðŸŸ  ORANGE)

**Impact:** Major functionality degraded, partial service disruption

**Examples:**
- Single replica down (HA still maintained)
- High replication lag (>5 minutes)
- API error rate >5%
- Database connection pool exhaustion
- High CPU/memory usage (>85%)

**Response Time:** 30 minutes
**Notification:** PagerDuty, Slack, Email

---

### P3 - Warning (ðŸŸ¡ YELLOW)

**Impact:** Minor degradation, potential future problems

**Examples:**
- Increased response times (p95 >500ms)
- Moderate resource usage (>70%)
- Slow database queries
- Cache hit rate below threshold
- Disk space warning (>80%)

**Response Time:** 2 hours (business hours), 4 hours (off-hours)
**Notification:** Slack, Email

---

### P4 - Info (ðŸ”µ BLUE)

**Impact:** Informational, no immediate action required

**Examples:**
- Successful deployments
- Scheduled maintenance
- Configuration changes
- Performance metrics reports

**Response Time:** Next business day
**Notification:** Email

---

## Escalation Matrix

### Level 1: On-Call Engineer (0-15 minutes)

**Responsibilities:**
- Acknowledge alert within 5 minutes
- Begin initial investigation
- Attempt immediate remediation for known issues
- Escalate if unable to resolve

**Skills Required:**
- System administration
- Basic troubleshooting
- Access to runbooks

---

### Level 2: Senior SRE (15-30 minutes)

**Trigger:** Level 1 cannot resolve within 15 minutes

**Responsibilities:**
- Deeper technical investigation
- Coordinate with development team if needed
- Make architectural decisions for quick fixes
- Update incident status

**Skills Required:**
- Deep system knowledge
- Database administration
- Kubernetes/infrastructure expertise

---

### Level 3: Engineering Lead (30-60 minutes)

**Trigger:** Level 2 cannot resolve within 30 minutes OR critical business impact

**Responsibilities:**
- Strategic decision making
- Resource allocation
- Coordinate cross-team efforts
- Communication with stakeholders
- Authorize emergency changes

**Skills Required:**
- System architecture expertise
- Business context understanding
- Incident command experience

---

### Level 4: CTO/VP Engineering (60+ minutes)

**Trigger:** Prolonged P1 incident (>1 hour) OR significant business impact (>$100K)

**Responsibilities:**
- Executive decision making
- External communication
- Vendor escalation
- Business continuity decisions

---

## Response Times (SLA)

| Severity | Acknowledgment | Initial Response | Resolution Target |
|----------|---------------|------------------|-------------------|
| **P1 - Critical** | 5 minutes | 15 minutes | 1 hour |
| **P2 - High** | 15 minutes | 30 minutes | 4 hours |
| **P3 - Warning** | 1 hour | 2 hours | 24 hours |
| **P4 - Info** | 4 hours | Next business day | No SLA |

### SLA Definitions

- **Acknowledgment:** Alert acknowledged in incident management system
- **Initial Response:** On-call engineer actively investigating
- **Resolution Target:** Issue resolved or workaround implemented

---

## Notification Channels

### PagerDuty (P1, P2)

**Integration:** Alertmanager â†’ PagerDuty API
**Recipients:** On-call rotation
**Escalation:** Automatic escalation after 15 minutes

**Service Keys:**
- `PAGERDUTY_SERVICE_KEY_CRITICAL` - P1 alerts
- `PAGERDUTY_SERVICE_KEY_DATABASE` - Database-specific
- `PAGERDUTY_SERVICE_KEY_HA` - High availability issues

### Opsgenie (Alternative)

**Integration:** Alertmanager â†’ Opsgenie API
**Recipients:** Teams and on-call schedules
**Priority Mapping:**
- P1 â†’ Opsgenie P1
- P2 â†’ Opsgenie P2
- P3 â†’ Opsgenie P3

### Slack

**Channels:**
- `#trading-system-alerts` - All alerts (P2-P4)
- `#trading-system-critical` - P1 alerts only
- `#security-alerts` - Security-related alerts

### Email

**Distribution Lists:**
- `trading-system-oncall@example.com` - On-call rotation
- `sre-team@example.com` - SRE team
- `engineering-leads@example.com` - Engineering leadership

---

## On-Call Rotation

### Schedule

**Primary On-Call:**
- **Week 1:** Engineer A
- **Week 2:** Engineer B
- **Week 3:** Engineer C
- **Week 4:** Engineer D

**Backup On-Call (escalation):**
- **Weeks 1-2:** Senior SRE 1
- **Weeks 3-4:** Senior SRE 2

**Coverage:**
- **Business Hours:** 9 AM - 6 PM (Monday-Friday)
- **After Hours:** 6 PM - 9 AM (weekdays), 24/7 (weekends)

### On-Call Expectations

1. **Availability:** Respond to pages within 5 minutes
2. **Tools Access:** Laptop with VPN and necessary credentials
3. **Documentation:** Update runbooks after each incident
4. **Handoff:** Provide detailed notes during rotation handoff

---

## Alert Categories

### 1. Application Alerts

**Components:** Trading system pods, dashboard, API
**Escalation Path:** On-Call â†’ Senior SRE â†’ Dev Team Lead

**Critical Alerts:**
- `TradingSystemNoReplicas`
- `TradingSystemPodCrashLooping`
- `HighErrorRate`

---

### 2. Database Alerts

**Components:** PostgreSQL primary, replicas
**Escalation Path:** On-Call â†’ Database Admin â†’ Senior SRE

**Critical Alerts:**
- `PostgreSQLPrimaryDown`
- `PostgreSQLNoReplicas`
- `PostgreSQLReplicationBroken`
- `DatabaseConnectionFailed`

---

### 3. Cache/State Alerts

**Components:** Redis master, replicas, Sentinel
**Escalation Path:** On-Call â†’ Senior SRE

**Critical Alerts:**
- `RedisMasterDown`
- `RedisSentinelQuorumLost`
- `RedisConnectionFailed`

---

### 4. Infrastructure Alerts

**Components:** Kubernetes nodes, load balancers, networking
**Escalation Path:** On-Call â†’ Infrastructure Team â†’ Cloud Support

**Critical Alerts:**
- `KubernetesNodeNotReady`
- `LoadBalancerNoHealthyBackends`
- `CriticalDiskUsage`

---

### 5. Security Alerts

**Components:** Authentication, authorization, intrusion detection
**Escalation Path:** On-Call â†’ Security Team â†’ CISO

**Critical Alerts:**
- `SuspiciousActivity`
- `UnauthorizedAccessAttempts`
- `SecurityViolation`

**Special Handling:** Security alerts trigger parallel notification to security team

---

### 6. Trading Operations Alerts

**Components:** Order execution, position management, risk controls
**Escalation Path:** On-Call â†’ Trading Ops â†’ Risk Manager â†’ Business Owner

**Critical Alerts:**
- `TradeExecutionFailureRate`
- `PortfolioRiskExceeded`
- `OrderExecutionFailed`

---

## Escalation Workflows

### Workflow 1: Critical Database Failure

```
1. Alert: PostgreSQLPrimaryDown
2. Level 1 (0-5 min):
   - Acknowledge alert
   - Check Grafana dashboards
   - Verify automatic failover initiated
   - Check replica status

3. If failover failed (5-10 min):
   - Execute manual failover runbook
   - Escalate to Database Admin

4. Level 2 (10-15 min):
   - Investigate root cause
   - Verify data consistency
   - Update application connection strings if needed

5. Post-incident (30+ min):
   - Root cause analysis
   - Update runbooks
   - Schedule post-mortem
```

---

### Workflow 2: High API Error Rate

```
1. Alert: HighErrorRate (>1% for 5 minutes)
2. Level 1 (0-5 min):
   - Check application logs
   - Identify error patterns
   - Check external API status (Zerodha)

3. If application issue (5-15 min):
   - Check recent deployments
   - Review error stack traces
   - Rollback if recent deployment suspected

4. If external API issue (5-10 min):
   - Enable fallback mechanisms
   - Notify trading operations
   - Contact vendor support

5. Level 2 (15-30 min):
   - Deep dive into logs
   - Check database connectivity
   - Review circuit breaker status
```

---

### Workflow 3: Security Alert

```
1. Alert: SuspiciousActivity OR UnauthorizedAccessAttempts
2. Immediate (0-2 min):
   - Page on-call engineer
   - Notify security team (parallel)
   - Capture current state

3. Investigation (2-15 min):
   - Review audit logs
   - Identify source IP addresses
   - Check for data exfiltration
   - Verify system integrity

4. Containment (15-30 min):
   - Block malicious IPs if confirmed
   - Rotate credentials if compromised
   - Isolate affected systems if needed

5. Security Team Takeover (30+ min):
   - Forensic analysis
   - Compliance notification
   - Incident report
```

---

## Post-Incident Procedures

### 1. Incident Documentation (Required for P1, P2)

**Timeline:** Within 24 hours of resolution

**Contents:**
- Incident summary
- Timeline of events
- Root cause analysis
- Impact assessment
- Resolution steps taken

**Tool:** Jira/ServiceNow incident ticket

---

### 2. Post-Mortem (Required for P1)

**Timeline:** Within 72 hours of resolution

**Attendees:**
- Incident responders
- Engineering leads
- Product owners
- Affected stakeholders

**Agenda:**
- What happened?
- What went well?
- What could be improved?
- Action items

**Deliverable:** Post-mortem document with action items

---

### 3. Runbook Updates

**Timeline:** Within 48 hours

**Requirements:**
- Document new troubleshooting steps
- Update recovery procedures
- Add lessons learned
- Review and approve changes

---

### 4. Metrics Tracking

**KPIs:**
- Mean Time to Acknowledge (MTTA)
- Mean Time to Resolution (MTTR)
- Alert fatigue metrics (false positive rate)
- SLA compliance

**Review:** Monthly SRE team meeting

---

## Alert Configuration Guidelines

### Creating New Alerts

**Checklist:**
1. âœ… Clear, actionable alert name
2. âœ… Appropriate severity level
3. âœ… Meaningful description and summary
4. âœ… Runbook reference
5. âœ… Proper threshold values (avoid false positives)
6. âœ… Test alert before production deployment

### Alert Hygiene

**Regular Reviews:** Quarterly
- Remove obsolete alerts
- Adjust thresholds based on historical data
- Update runbooks
- Review false positive rates

---

## Emergency Contacts

### Primary Contacts

| Role | Contact | Phone | Email | Hours |
|------|---------|-------|-------|-------|
| On-Call Engineer | See PagerDuty | - | oncall@example.com | 24/7 |
| Senior SRE | John Doe | +1-555-0101 | john.doe@example.com | Business hours |
| Database Admin | Jane Smith | +1-555-0102 | jane.smith@example.com | Business hours |
| Engineering Lead | Bob Johnson | +1-555-0103 | bob.johnson@example.com | Business hours |
| CTO | Alice Williams | +1-555-0104 | alice.williams@example.com | Emergencies only |

### Vendor Support

| Vendor | Support Type | Contact | SLA |
|--------|--------------|---------|-----|
| Zerodha | API Issues | support@zerodha.com | 2 hours |
| AWS | Infrastructure | AWS Support Console | 1 hour (Premium) |
| PagerDuty | Alert System | support@pagerduty.com | 4 hours |

---

## Compliance & Audit

### Audit Requirements

- All P1/P2 incidents must be documented
- Monthly SLA compliance reports
- Quarterly escalation policy review
- Annual disaster recovery drill

### Compliance Standards

- SOC 2 Type II
- SEBI regulatory requirements
- Internal audit standards

---

## Revision History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | Nov 2025 | SRE Team | Initial version |

---

## Appendix

### Useful Links

- [Prometheus Dashboard](http://prometheus.example.com)
- [Grafana Dashboards](http://grafana.example.com)
- [PagerDuty Console](https://yourcompany.pagerduty.com)
- [Runbook Repository](https://github.com/yourcompany/trading-system-runbooks)
- [Incident Management](https://jira.example.com/incidents)

### Related Documents

- [Disaster Recovery Plan](./DISASTER_RECOVERY.md)
- [Runbook Index](./RUNBOOKS.md)
- [High Availability Architecture](./HA_ARCHITECTURE.md)
- [Security Incident Response](./SECURITY_INCIDENT_RESPONSE.md)

---

**Document Classification:** Internal
**Review Frequency:** Quarterly
**Next Review Date:** February 2026
