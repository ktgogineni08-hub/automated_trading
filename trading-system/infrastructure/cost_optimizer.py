#!/usr/bin/env python3
"""
Cost Optimization Module
Reduces operational costs through intelligent caching, batching, and resource optimization

TARGET: 30-35% cost reduction ($250/month savings)

OPTIMIZATION STRATEGIES:
1. Intelligent API Call Caching (30-40% reduction in API calls)
2. Request Deduplication (prevent duplicate API calls)
3. Batch Processing (reduce API call overhead)
4. Connection Pool Optimization (reduce connection overhead)
5. Query Result Caching (reduce database load)
6. Off-Peak Processing (leverage cheaper computing hours)
7. Memory-Efficient Data Structures (reduce memory costs)
"""

import logging
import time
import hashlib
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import OrderedDict, deque
from enum import Enum
import threading
import asyncio
from functools import wraps

logger = logging.getLogger('trading_system.cost_optimizer')


class CacheStrategy(Enum):
    """Cache eviction strategies"""
    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    TTL = "ttl"  # Time To Live


@dataclass
class CacheEntry:
    """Cache entry with metadata"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    ttl_seconds: Optional[int] = None
    cost_saved: float = 0.0  # Estimated cost saved by caching

    def is_expired(self) -> bool:
        """Check if entry has expired"""
        if self.ttl_seconds is None:
            return False
        age = (datetime.now() - self.created_at).total_seconds()
        return age > self.ttl_seconds

    def update_access(self):
        """Update access metadata"""
        self.last_accessed = datetime.now()
        self.access_count += 1


@dataclass
class CostMetrics:
    """Cost tracking metrics"""
    api_calls_saved: int = 0
    api_calls_made: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    queries_cached: int = 0
    requests_deduplicated: int = 0
    batches_processed: int = 0
    estimated_cost_saved: float = 0.0
    estimated_cost_spent: float = 0.0

    def get_cache_hit_rate(self) -> float:
        """Calculate cache hit rate"""
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total * 100) if total > 0 else 0.0

    def get_cost_reduction_pct(self) -> float:
        """Calculate cost reduction percentage"""
        total = self.estimated_cost_saved + self.estimated_cost_spent
        return (self.estimated_cost_saved / total * 100) if total > 0 else 0.0


class IntelligentCache:
    """
    Intelligent multi-level cache with cost tracking

    Features:
    - Multiple eviction strategies (LRU, LFU, TTL)
    - Automatic TTL based on data volatility
    - Cost tracking per cache entry
    - Cache warming for frequently accessed data
    - Memory-efficient storage
    """

    def __init__(
        self,
        max_size: int = 10000,
        default_ttl: int = 300,  # 5 minutes
        strategy: CacheStrategy = CacheStrategy.LRU
    ):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.strategy = strategy

        # Cache storage
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._lock = threading.RLock()

        # Metrics
        self.metrics = CostMetrics()

        logger.info(f"ðŸ’¾ IntelligentCache initialized (max_size={max_size}, strategy={strategy.value})")

    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        with self._lock:
            if key not in self._cache:
                self.metrics.cache_misses += 1
                return None

            entry = self._cache[key]

            # Check expiration
            if entry.is_expired():
                del self._cache[key]
                self.metrics.cache_misses += 1
                return None

            # Update access metadata
            entry.update_access()
            self.metrics.cache_hits += 1
            self.metrics.estimated_cost_saved += entry.cost_saved

            # Move to end for LRU
            if self.strategy == CacheStrategy.LRU:
                self._cache.move_to_end(key)

            return entry.value

    def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        cost_saved: float = 0.0
    ):
        """Set value in cache"""
        with self._lock:
            # Create entry
            entry = CacheEntry(
                key=key,
                value=value,
                created_at=datetime.now(),
                last_accessed=datetime.now(),
                ttl_seconds=ttl or self.default_ttl,
                cost_saved=cost_saved
            )

            # Add to cache
            self._cache[key] = entry

            # Evict if over capacity
            if len(self._cache) > self.max_size:
                self._evict()

    def _evict(self):
        """Evict entries based on strategy"""
        if self.strategy == CacheStrategy.LRU:
            # Remove oldest (first item in OrderedDict)
            self._cache.popitem(last=False)

        elif self.strategy == CacheStrategy.LFU:
            # Remove least frequently used
            min_key = min(self._cache.keys(), key=lambda k: self._cache[k].access_count)
            del self._cache[min_key]

        elif self.strategy == CacheStrategy.TTL:
            # Remove expired entries first, then oldest
            expired = [k for k, v in self._cache.items() if v.is_expired()]
            if expired:
                del self._cache[expired[0]]
            else:
                self._cache.popitem(last=False)

    def invalidate(self, key: str):
        """Invalidate cache entry"""
        with self._lock:
            if key in self._cache:
                del self._cache[key]

    def clear(self):
        """Clear entire cache"""
        with self._lock:
            self._cache.clear()

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self._lock:
            return {
                'size': len(self._cache),
                'max_size': self.max_size,
                'hit_rate': self.metrics.get_cache_hit_rate(),
                'hits': self.metrics.cache_hits,
                'misses': self.metrics.cache_misses,
                'cost_saved': self.metrics.estimated_cost_saved
            }


class RequestDeduplicator:
    """
    Deduplicates concurrent requests to prevent redundant API calls

    If multiple parts of the system request the same data simultaneously,
    only one API call is made and the result is shared with all requesters.
    """

    def __init__(self):
        self._pending_requests: Dict[str, asyncio.Future] = {}
        self._lock = asyncio.Lock()
        self.requests_deduplicated = 0

    async def deduplicate(self, key: str, fetch_fn: Callable) -> Any:
        """
        Deduplicate request

        Args:
            key: Unique request identifier
            fetch_fn: Async function to fetch data

        Returns:
            Result from fetch function (shared across duplicate requests)
        """
        async with self._lock:
            # Check if request is already pending
            if key in self._pending_requests:
                self.requests_deduplicated += 1
                logger.debug(f"Deduplicating request: {key}")
                # Wait for existing request to complete
                return await self._pending_requests[key]

            # Create new future for this request
            future = asyncio.Future()
            self._pending_requests[key] = future

        try:
            # Execute fetch function
            result = await fetch_fn()
            future.set_result(result)
            return result

        except Exception as e:
            future.set_exception(e)
            raise

        finally:
            # Remove from pending
            async with self._lock:
                del self._pending_requests[key]


class BatchProcessor:
    """
    Batches multiple operations to reduce API call overhead

    Accumulates requests over a time window and processes them in a single batch,
    reducing API calls and network overhead.
    """

    def __init__(
        self,
        batch_size: int = 10,
        batch_timeout: float = 1.0,  # seconds
        process_fn: Optional[Callable] = None
    ):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.process_fn = process_fn

        self._batch_queue: List[Tuple[str, Any, asyncio.Future]] = []
        self._lock = asyncio.Lock()
        self._batch_task = None
        self._running = False

        self.batches_processed = 0
        self.items_batched = 0

    async def add(self, key: str, item: Any) -> Any:
        """
        Add item to batch

        Args:
            key: Item identifier
            item: Item data

        Returns:
            Processing result
        """
        future = asyncio.Future()

        async with self._lock:
            self._batch_queue.append((key, item, future))

            # Start batch timer if not running
            if not self._running:
                self._running = True
                self._batch_task = asyncio.create_task(self._process_batch_delayed())

            # Process immediately if batch is full
            if len(self._batch_queue) >= self.batch_size:
                await self._process_batch()

        return await future

    async def _process_batch_delayed(self):
        """Process batch after timeout"""
        await asyncio.sleep(self.batch_timeout)

        async with self._lock:
            if self._batch_queue:
                await self._process_batch()
            self._running = False

    async def _process_batch(self):
        """Process current batch"""
        if not self._batch_queue:
            return

        batch = self._batch_queue[:]
        self._batch_queue.clear()

        self.batches_processed += 1
        self.items_batched += len(batch)

        logger.debug(f"Processing batch of {len(batch)} items")

        try:
            if self.process_fn:
                # Process entire batch
                items = [item for _, item, _ in batch]
                results = await self.process_fn(items)

                # Set results for each future
                for (_, _, future), result in zip(batch, results):
                    future.set_result(result)
            else:
                # No processor, just return items
                for key, item, future in batch:
                    future.set_result(item)

        except Exception as e:
            # Set exception for all futures
            for _, _, future in batch:
                future.set_exception(e)


class CostOptimizer:
    """
    Comprehensive Cost Optimization System

    Integrates multiple cost-saving strategies:
    - API call caching
    - Request deduplication
    - Batch processing
    - Query caching
    - Off-peak scheduling

    Usage:
        optimizer = CostOptimizer()

        # Cache API calls
        @optimizer.cache_api_call(ttl=300)
        async def get_quote(symbol):
            return await api.get_quote(symbol)

        # Deduplicate concurrent requests
        result = await optimizer.deduplicate_request(
            key='quote:NIFTY',
            fetch_fn=lambda: api.get_quote('NIFTY')
        )

        # Batch operations
        await optimizer.batch_operation('symbol_update', symbol_data)
    """

    # Cost constants (estimated costs per operation)
    COST_PER_API_CALL = 0.001  # $0.001 per API call
    COST_PER_DB_QUERY = 0.0001  # $0.0001 per query

    def __init__(
        self,
        cache_size: int = 10000,
        cache_ttl: int = 300,
        batch_size: int = 10,
        batch_timeout: float = 1.0
    ):
        # Initialize components
        self.cache = IntelligentCache(max_size=cache_size, default_ttl=cache_ttl)
        self.deduplicator = RequestDeduplicator()
        self.batch_processors: Dict[str, BatchProcessor] = {}

        # Global metrics
        self.metrics = CostMetrics()

        # Lock for thread safety
        self._lock = threading.RLock()

        logger.info("ðŸ’° CostOptimizer initialized")

    def cache_api_call(self, ttl: Optional[int] = None, key_fn: Optional[Callable] = None):
        """
        Decorator to cache API call results

        Args:
            ttl: Cache TTL in seconds
            key_fn: Function to generate cache key from arguments

        Usage:
            @optimizer.cache_api_call(ttl=300)
            async def get_quote(symbol):
                return await api.get_quote(symbol)
        """
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Generate cache key
                if key_fn:
                    cache_key = key_fn(*args, **kwargs)
                else:
                    cache_key = self._generate_cache_key(func.__name__, args, kwargs)

                # Check cache
                cached_value = self.cache.get(cache_key)
                if cached_value is not None:
                    self.metrics.api_calls_saved += 1
                    logger.debug(f"Cache hit: {cache_key}")
                    return cached_value

                # Call function
                result = await func(*args, **kwargs)

                # Cache result
                self.cache.set(
                    key=cache_key,
                    value=result,
                    ttl=ttl,
                    cost_saved=self.COST_PER_API_CALL
                )

                self.metrics.api_calls_made += 1
                self.metrics.estimated_cost_spent += self.COST_PER_API_CALL

                return result

            return wrapper
        return decorator

    def _generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """Generate cache key from function name and arguments"""
        key_parts = [func_name]

        # Add positional args
        for arg in args:
            key_parts.append(str(arg))

        # Add keyword args (sorted for consistency)
        for k, v in sorted(kwargs.items()):
            key_parts.append(f"{k}={v}")

        key_str = ":".join(key_parts)

        # Hash if too long
        if len(key_str) > 200:
            return hashlib.md5(key_str.encode()).hexdigest()

        return key_str

    async def deduplicate_request(self, key: str, fetch_fn: Callable) -> Any:
        """
        Deduplicate concurrent requests

        Args:
            key: Request identifier
            fetch_fn: Async function to fetch data

        Returns:
            Fetch result (shared if duplicate)
        """
        result = await self.deduplicator.deduplicate(key, fetch_fn)
        self.metrics.requests_deduplicated = self.deduplicator.requests_deduplicated
        return result

    async def batch_operation(
        self,
        batch_key: str,
        item: Any,
        process_fn: Optional[Callable] = None
    ) -> Any:
        """
        Add operation to batch

        Args:
            batch_key: Batch identifier
            item: Item to batch
            process_fn: Optional batch processor function

        Returns:
            Processing result
        """
        # Create batch processor if doesn't exist
        if batch_key not in self.batch_processors:
            self.batch_processors[batch_key] = BatchProcessor(process_fn=process_fn)

        processor = self.batch_processors[batch_key]
        result = await processor.add(batch_key, item)

        # Update metrics
        self.metrics.batches_processed = sum(
            p.batches_processed for p in self.batch_processors.values()
        )

        return result

    def cache_query_result(
        self,
        query_key: str,
        result: Any,
        ttl: int = 60
    ):
        """
        Cache database query result

        Args:
            query_key: Query identifier
            result: Query result
            ttl: Cache TTL
        """
        self.cache.set(
            key=f"query:{query_key}",
            value=result,
            ttl=ttl,
            cost_saved=self.COST_PER_DB_QUERY
        )
        self.metrics.queries_cached += 1

    def get_cached_query(self, query_key: str) -> Optional[Any]:
        """
        Get cached query result

        Args:
            query_key: Query identifier

        Returns:
            Cached result or None
        """
        return self.cache.get(f"query:{query_key}")

    def is_off_peak_hours(self) -> bool:
        """
        Check if current time is off-peak hours (cheaper computing)

        Off-peak: 10 PM - 6 AM (typically lower cloud costs)
        """
        current_hour = datetime.now().hour
        return current_hour >= 22 or current_hour < 6

    def should_defer_to_off_peak(self, priority: str = "low") -> bool:
        """
        Determine if operation should be deferred to off-peak

        Args:
            priority: Operation priority (low, medium, high, critical)

        Returns:
            True if should defer
        """
        if priority in ["high", "critical"]:
            return False

        return not self.is_off_peak_hours()

    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive cost metrics"""
        cache_stats = self.cache.get_stats()

        return {
            'api_calls_made': self.metrics.api_calls_made,
            'api_calls_saved': self.metrics.api_calls_saved,
            'cache_hit_rate': cache_stats['hit_rate'],
            'requests_deduplicated': self.metrics.requests_deduplicated,
            'batches_processed': self.metrics.batches_processed,
            'queries_cached': self.metrics.queries_cached,
            'estimated_cost_spent': self.metrics.estimated_cost_spent,
            'estimated_cost_saved': self.cache.metrics.estimated_cost_saved,
            'cost_reduction_pct': self.metrics.get_cost_reduction_pct(),
            'cache_size': cache_stats['size'],
            'cache_max_size': cache_stats['max_size']
        }

    def print_metrics(self):
        """Print cost optimization metrics"""
        metrics = self.get_metrics()

        print("\n" + "="*60)
        print("ðŸ’° COST OPTIMIZATION METRICS")
        print("="*60)
        print(f"API Calls Made:          {metrics['api_calls_made']:,}")
        print(f"API Calls Saved:         {metrics['api_calls_saved']:,}")
        print(f"Cache Hit Rate:          {metrics['cache_hit_rate']:.1f}%")
        print(f"Requests Deduplicated:   {metrics['requests_deduplicated']:,}")
        print(f"Batches Processed:       {metrics['batches_processed']:,}")
        print(f"Queries Cached:          {metrics['queries_cached']:,}")
        print(f"\nCost Spent:              ${metrics['estimated_cost_spent']:.2f}")
        print(f"Cost Saved:              ${metrics['estimated_cost_saved']:.2f}")
        print(f"Cost Reduction:          {metrics['cost_reduction_pct']:.1f}%")
        print(f"\nCache Usage:             {metrics['cache_size']:,} / {metrics['cache_max_size']:,}")
        print("="*60 + "\n")


# Global cost optimizer instance
_global_cost_optimizer: Optional[CostOptimizer] = None


def get_cost_optimizer() -> CostOptimizer:
    """Get global cost optimizer instance (singleton)"""
    global _global_cost_optimizer
    if _global_cost_optimizer is None:
        _global_cost_optimizer = CostOptimizer()
    return _global_cost_optimizer


if __name__ == "__main__":
    # Test cost optimizer
    import asyncio

    async def test_optimizer():
        print("Testing Cost Optimizer...\n")

        optimizer = CostOptimizer()

        # Test 1: API call caching
        print("Test 1: API Call Caching")

        @optimizer.cache_api_call(ttl=60)
        async def mock_api_call(symbol: str):
            await asyncio.sleep(0.1)  # Simulate API delay
            return {"symbol": symbol, "price": 25000}

        # First call - cache miss
        start = time.time()
        result1 = await mock_api_call("NIFTY")
        time1 = time.time() - start
        print(f"  First call: {time1*1000:.1f}ms - {result1}")

        # Second call - cache hit (should be much faster)
        start = time.time()
        result2 = await mock_api_call("NIFTY")
        time2 = time.time() - start
        print(f"  Cached call: {time2*1000:.1f}ms - {result2}")
        print(f"  Speedup: {time1/time2:.1f}x faster\n")

        # Test 2: Request deduplication
        print("Test 2: Request Deduplication")

        async def slow_fetch():
            await asyncio.sleep(0.2)
            return {"data": "expensive_result"}

        # Launch 5 concurrent identical requests
        tasks = [
            optimizer.deduplicate_request("test:data", slow_fetch)
            for _ in range(5)
        ]
        start = time.time()
        results = await asyncio.gather(*tasks)
        duration = time.time() - start

        print(f"  5 concurrent requests completed in {duration*1000:.1f}ms")
        print(f"  Requests deduplicated: {optimizer.deduplicator.requests_deduplicated}")
        print(f"  All results identical: {len(set(str(r) for r in results)) == 1}\n")

        # Test 3: Batch processing
        print("Test 3: Batch Processing")

        async def batch_process(items):
            # Simulate batch API call
            await asyncio.sleep(0.1)
            return [{"item": item, "processed": True} for item in items]

        # Add items to batch
        batch_results = []
        for i in range(15):
            result = await optimizer.batch_operation("test_batch", f"item_{i}", batch_process)
            batch_results.append(result)

        print(f"  Processed {len(batch_results)} items")
        print(f"  Batches created: {optimizer.batch_processors['test_batch'].batches_processed}\n")

        # Print metrics
        optimizer.print_metrics()

        print("âœ… Cost optimizer tests passed")

    asyncio.run(test_optimizer())
